{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce280679-aee9-4c41-97c0-f6e8a354f401",
   "metadata": {},
   "source": [
    "# PyTorch for Natural Language Processing\n",
    "\n",
    "## Bot Detection using BERTModel\n",
    "\n",
    "---\n",
    "\n",
    "**<u>_Objective:_</u>** In this short project, we fine-tune a BERT pretrained model to classify tweets made by a bot, or by a human.\n",
    "\n",
    "This tutorial is inspired by the following walkthrough:\n",
    "\n",
    "https://saturncloud.io/blog/pytorch-for-natural-language-processing-building-a-fake-news-classification-model/\n",
    "\n",
    "\n",
    "### Introduction \n",
    "\n",
    "Bot detection lols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ab01a-65ee-42dd-a9e6-9dc3fd4fd3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb68be-df1f-454c-b327-0690038843a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c873fb4d-c6d1-4b41-8897-8bc16a9fb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_theme(style = 'whitegrid', \n",
    "              rc    = {'figure.dpi'    : 400, \n",
    "                       'figure.figsize': (20, 12)}, \n",
    "              font_scale = 0.60)\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "warnings.filterwarnings('ignore', category = UserWarning, module = 'openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a7bca-33ed-44ec-829a-f7d18e808f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b491c0bf-d7e6-4345-9632-e9a7e83258f9",
   "metadata": {},
   "source": [
    "## Set up Environment for Google Colab\n",
    "\n",
    "You would need to have GPU to run the fine-tuning. Google Colab is a great platform for using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c0f14b-019f-48ef-b154-a22e78af6ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "384eae10-c2fb-4b02-b893-564e6e9bee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: i:\\My Drive\\Data Science and Analytics Portfolio\\3 Tutorials\\2_Bots_Detection\\2_Notebooks\n",
      "Root directory : i:\\My Drive\\Data Science and Analytics Portfolio\\3 Tutorials\\2_Bots_Detection\\\n",
      "Data directory : i:\\My Drive\\Data Science and Analytics Portfolio\\3 Tutorials\\2_Bots_Detection\\1_Data\\\n",
      "Model directory : i:\\My Drive\\Data Science and Analytics Portfolio\\3 Tutorials\\2_Bots_Detection\\3_Model\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get current and root directory\n",
    "cur_dir = os.getcwd()\n",
    "root_dir = cur_dir[:-11]\n",
    "data_dir = root_dir + \"1_Data\\\\\"\n",
    "model_dir = root_dir + \"3_Model\\\\\"\n",
    "\n",
    "print(f\"Current directory: {cur_dir}\\nRoot directory : {root_dir}\\nData directory : {data_dir}\\nModel directory : {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db240a6b-d127-4470-adce-43afec715e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "619a21bc-d1f2-4841-a5d4-aedfec4b93b6",
   "metadata": {},
   "source": [
    "## Read Datasets\n",
    "\n",
    "For the purpose of this project, we wil only take a sample of about 5000 tweets from both dataframes, so that the training does not take too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11db2ba2-e587-4bbd-aa62-be93987a5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_human_temp = pd.read_csv(f\"{data_dir}/cresci-2015/TFP_tweets.csv\", encoding = 'latin-1')\n",
    "df_bot_temp = pd.read_csv(f\"{data_dir}/cresci-2015/TWT_tweets.csv\", encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6797d47-94ab-445c-bd52-f086c215ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of human dataframe : 563693\n",
      "Legnth of bots dataframe : 114192\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of human dataframe : {len(df_human_temp)}\\nLegnth of bots dataframe : {len(df_bot_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7657d4-e081-4c5a-a93c-5244fb035e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9975cc20-5480-4718-84b8-cda446590c86",
   "metadata": {},
   "source": [
    "The size of the two datasets are very huge. What we can do, is to split them up into two datasets - one that will go into the train-test-split function, and another that will be used for model prediction. For the former, we only select about 5000 rows of them, as so that the model does not take too long to train. Generally speaking, the more data, the better the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10a3b87-f05b-419a-b39c-cb9c243b64e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of human sample : 5000\n",
      "Legnth of bots sample : 6000\n",
      "Length of train dataframe : 11000\n",
      "Length of evaluation dataframe : 1900\n"
     ]
    }
   ],
   "source": [
    "## Split into training data\n",
    "df_human_train_sample = df_human_temp.sample(n = 5000, random_state = 37)\n",
    "df_bot_train_sample = df_bot_temp.sample(n = 6000, random_state = 37)\n",
    "\n",
    "# Those tweets that are not inside the training dataset, are for predictions. Again, we only pick about 1000\n",
    "df_human_eval =  df_human_temp[~df_human_temp['id'].isin(df_human_train_sample['id'].values)]\n",
    "df_bot_eval = df_bot_temp[~df_bot_temp['id'].isin(df_bot_train_sample['id'].values)]\n",
    "\n",
    "# Select only 1000 tweets for the evaluation dataset\n",
    "df_human_eval_sample = df_human_eval.sample(n = 1000, random_state = 30)\n",
    "df_bot_eval_sample = df_bot_eval.sample(n = 900, random_state = 30)\n",
    "\n",
    "# Then we only select the relevant columns and encode the humans as 0 and bots as 1\n",
    "df_human_train_sample['target'] = 0\n",
    "df_bot_train_sample['target'] = 1\n",
    "\n",
    "df_human_eval_sample['target'] = 0\n",
    "df_bot_eval_sample['target'] = 1\n",
    "\n",
    "# Vstack the dataframes together, and randomly shuffle them dataframe\n",
    "df_train_sample = pd.concat([df_human_train_sample, df_bot_train_sample], axis = 0, ignore_index = True) \n",
    "df_eval_sample = pd.concat([df_human_eval_sample, df_bot_eval_sample], axis = 0, ignore_index = True) \n",
    "\n",
    "df_train_sample = df_train_sample.sample(frac = 1.0, random_state = 90)\n",
    "df_eval_sample = df_eval_sample.sample(frac = 1.0, random_state = 90)\n",
    "\n",
    "\n",
    "print(f\"Length of human sample : {len(df_human_train_sample)}\\nLegnth of bots sample : {len(df_bot_train_sample)}\\nLength of train dataframe : {len(df_train_sample)}\")\n",
    "print(f\"Length of evaluation dataframe : {len(df_eval_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b319882-7ee5-470a-b507-12ee01f449c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4034ea-af94-4328-b429-b8aff358fd78",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Usually, we want to perform some rudimentary data cleaning steps on the dataset before we use it for training. Typically, this involves:\n",
    "- Removing special characters\n",
    "- Lower case all letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a46898c-b1db-4200-9c48-c15401eb7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    \n",
    "    tweet1 = re.sub('[^A-Za-z0-9]+', ' ', tweet)\n",
    "    tweet2 = tweet1.lower()\n",
    "    tweet3 = tweet2.strip()\n",
    "    \n",
    "    return tweet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f66d0ae-3e6c-4de2-b11a-b3081b26a855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>179273485179297793</td>\n",
       "      <td>#FiatRom Caro Marchionne, il capitale e' sempr...</td>\n",
       "      <td>0</td>\n",
       "      <td>fiatrom caro marchionne il capitale e sempre s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>290532928440639488</td>\n",
       "      <td>Roma/ A #Ostia si continua a sparare e la poli...</td>\n",
       "      <td>0</td>\n",
       "      <td>roma a ostia si continua a sparare e la polizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7656</th>\n",
       "      <td>327948536136204288</td>\n",
       "      <td>http://t.co/ivA8bzYixB ÑÐ°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿...</td>\n",
       "      <td>1</td>\n",
       "      <td>http t co iva8bzyixb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>292313083421011970</td>\n",
       "      <td>@Diabolikart Quoto su tutta la linea ;-))) bac...</td>\n",
       "      <td>0</td>\n",
       "      <td>diabolikart quoto su tutta la linea bacio e ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>110463456469188608</td>\n",
       "      <td>@midbrito foi na pria ontem&amp;gt;?</td>\n",
       "      <td>1</td>\n",
       "      <td>midbrito foi na pria ontem gt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10919</th>\n",
       "      <td>301939393281798144</td>\n",
       "      <td>TODO MEXICO APOYA Y PIDE  SE RESPETE LA DECISI...</td>\n",
       "      <td>1</td>\n",
       "      <td>todo mexico apoya y pide se respete la decisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>13304808760938497</td>\n",
       "      <td>@sigatchegarotos queria dar os parabÃ©ns a vcs...</td>\n",
       "      <td>1</td>\n",
       "      <td>sigatchegarotos queria dar os parab ns a vcs p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>135782553570385920</td>\n",
       "      <td>@euphonik thank u for the brilliant tracks !!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>euphonik thank u for the brilliant tracks help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>311160499553333248</td>\n",
       "      <td>RT @WIPO: Free ePCT webinars: Learn how to man...</td>\n",
       "      <td>0</td>\n",
       "      <td>rt wipo free epct webinars learn how to manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>125867823514910720</td>\n",
       "      <td>Il novanta per cento della fantascienza ÃÂ¨ s...</td>\n",
       "      <td>0</td>\n",
       "      <td>il novanta per cento della fantascienza spazza...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10739 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "2217   179273485179297793  #FiatRom Caro Marchionne, il capitale e' sempr...   \n",
       "3387   290532928440639488  Roma/ A #Ostia si continua a sparare e la poli...   \n",
       "7656   327948536136204288  http://t.co/ivA8bzYixB ÑÐ°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿...   \n",
       "1833   292313083421011970  @Diabolikart Quoto su tutta la linea ;-))) bac...   \n",
       "7497   110463456469188608                   @midbrito foi na pria ontem&gt;?   \n",
       "...                   ...                                                ...   \n",
       "10919  301939393281798144  TODO MEXICO APOYA Y PIDE  SE RESPETE LA DECISI...   \n",
       "9539    13304808760938497  @sigatchegarotos queria dar os parabÃ©ns a vcs...   \n",
       "6815   135782553570385920  @euphonik thank u for the brilliant tracks !!!...   \n",
       "2717   311160499553333248  RT @WIPO: Free ePCT webinars: Learn how to man...   \n",
       "3931   125867823514910720  Il novanta per cento della fantascienza ÃÂ¨ s...   \n",
       "\n",
       "       target                                       text_cleaned  \n",
       "2217        0  fiatrom caro marchionne il capitale e sempre s...  \n",
       "3387        0  roma a ostia si continua a sparare e la polizi...  \n",
       "7656        1                               http t co iva8bzyixb  \n",
       "1833        0  diabolikart quoto su tutta la linea bacio e ne...  \n",
       "7497        1                      midbrito foi na pria ontem gt  \n",
       "...       ...                                                ...  \n",
       "10919       1  todo mexico apoya y pide se respete la decisio...  \n",
       "9539        1  sigatchegarotos queria dar os parab ns a vcs p...  \n",
       "6815        1  euphonik thank u for the brilliant tracks help...  \n",
       "2717        0  rt wipo free epct webinars learn how to manage...  \n",
       "3931        0  il novanta per cento della fantascienza spazza...  \n",
       "\n",
       "[10739 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train_sample[['id', 'text', 'target']].copy()\n",
    "df_eval = df_eval_sample[['id', 'text', 'target']].copy()\n",
    "\n",
    "df_train['text_cleaned'] = df_train['text'].apply(lambda x: clean_text(x)) \n",
    "df_eval['text_cleaned'] = df_eval['text'].apply(lambda x: clean_text(x)) \n",
    "\n",
    "# Remove any rows that are blanks\n",
    "df_train = df_train[df_train['text_cleaned'] != '']\n",
    "df_eval = df_eval[df_eval['text_cleaned'] != '']\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca94ec60-d5ed-4d18-b25b-0b56e26f06f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5739\n",
       "0    5000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d24e1c-fbfb-4b42-ba2d-459e9c4b0fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    999\n",
       "1    858\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b76010-ca8f-4e77-a79c-3785ad447580",
   "metadata": {},
   "source": [
    "Now, we are ready to send the dataframe into the train-test-split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41741a59-b080-4d79-8d5a-7e2094c200b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train : 8591, Length of X_test: 2148\n",
      "Length of y_train : 8591, Length of y_test: 2148\n"
     ]
    }
   ],
   "source": [
    "texts = df_train['text'].values\n",
    "targets = df_train['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, targets, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(f\"Length of X_train : {len(X_train)}, Length of X_test: {len(X_test)}\\nLength of y_train : {len(y_train)}, Length of y_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d808c-1c8e-4352-8db5-825089902219",
   "metadata": {},
   "source": [
    "As we can see, the lengths of the features (X) and their corresponding labels (y) are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bff494-745e-47ca-9728-8c4329bc7dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcbd11-8793-4d2f-8b28-0efa21e876a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97eb1f57-1391-437f-9730-ce24761bdeb0",
   "metadata": {},
   "source": [
    "## Parameters Declarations\n",
    "\n",
    "In this section, we want to declare some variables that we will need in our model training latter\n",
    "\n",
    "There are many bert models you can try. See the list of models hosted by Hugging Face on the link below:\n",
    "\n",
    "https://huggingface.co/google-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de642731-e7b7-4bba-bb2b-1925e8490067",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-uncased'  # Name of the bert model\n",
    "batch_size      = 8                    # Size of each batch that the dataloader will send into BERT for training\n",
    "learning_rate   = 2e-5                 # Learning rate of Bert\n",
    "best_accuracy   = 1.000                # Initiate best accuracy of the model\n",
    "num_epoch       = 10                   # Number of epoches for the training i.e. total number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62fbeb-9807-4720-83a1-ea5d40670252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430f71f-aac2-4546-844f-90f2c22251e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef599042-26e1-460f-be8f-36d7cdda2e94",
   "metadata": {},
   "source": [
    "## Specify GPU\n",
    "\n",
    "We need to have a GPU to push the model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "465d636e-9afa-4cd3-ae85-c449eceb53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu \n",
      "Name of device: CPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "try:\n",
    "    device_name = torch.cuda.get_device_name()\n",
    "except:\n",
    "    device_name = 'CPU'\n",
    "print(device, '\\nName of device:', device_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891503e6-3ead-48cb-9be9-7a33fdbde5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6d34cd3-f32c-4933-b370-78367ccd1469",
   "metadata": {},
   "source": [
    "## Tokenize Texts\n",
    "\n",
    "In order for BERT to perform its embeddings and classification, we have to split the sentences into individual words - or _tokens_. In BERT, there is a tokenizer we can use to do just this. Stil step of tokenization will take some time, depending on the compute RAM as well as how long the sentences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f4c338-2980-45a1-88eb-ba4b01ac928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case = True)\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation = True, padding = True, max_length = 128) \n",
    "test_encodings = tokenizer(list(X_test), truncation = True, padding = True, max_length = 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a396749-d32a-41c6-8c09-566df645abb2",
   "metadata": {},
   "source": [
    "We can also check what does the tokenizer returns us. Let's pick the ```train_encodings```. The encodings actually return us a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f310325-ce34-46c7-8dbb-9b6d635fbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Length of input_ids : 8591\n",
      "Length of token_type_ids : 8591\n",
      "Length of attention_mask : 8591\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings.keys())\n",
    "for key in train_encodings.keys():\n",
    "    print(f\"Length of {key} : {len(train_encodings[key])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501acd61-9073-4715-850a-4d15819fc181",
   "metadata": {},
   "source": [
    "Note that this is exactly the same length of X_train! We can examine each of the keys and see what do they represent. We pick only the first text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6004953-20ee-470c-9d57-bb10c9e09066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each input_ids : 128\n",
      "[101, 9594, 3762, 1005, 1055, 2047, 1001, 9121, 1012, 1012, 1012, 8299, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 22851, 8093, 2102, 2487, 2015, 2078, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of each input_ids : {len(train_encodings['input_ids'][0])}\\n{train_encodings['input_ids'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4aee01",
   "metadata": {},
   "source": [
    "Each of the ```input_ids``` are exactly 128 long - this is because we have set such as value a the ```max_length``` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01dd5e7-6015-4b35-8f53-e5b9f28c401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each token_type_ids : 128\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of each token_type_ids : {len(train_encodings['token_type_ids'][0])}\\n{train_encodings['token_type_ids'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e276e",
   "metadata": {},
   "source": [
    "lastly, we have the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b636cd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each token_type_ids : 128\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of each token_type_ids : {len(train_encodings['attention_mask'][0])}\\n{train_encodings['attention_mask'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ba602",
   "metadata": {},
   "source": [
    "Actaully, out of the three keys, only ```input_ids``` and ```attention_mask``` are required, as well as the target values, for fine-tuning the model. Next, we create another function to convert the lists to PyTorch Tensors, and sending them to the GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b01c6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_device(encodings, target, device):\n",
    "\n",
    "    inputs = torch.tensor(encodings['input_ids']).to(device)\n",
    "    masks = torch.tensor(encodings['attention_mask']).to(device)\n",
    "    labels = torch.tensor(target).to(device)\n",
    "    return inputs, masks, labels\n",
    "\n",
    "# Function call\n",
    "train_inputs, train_masks, train_labels = convert_tensor_device(train_encodings, y_train, device)\n",
    "test_inputs, test_masks, test_labels = convert_tensor_device(test_encodings, y_test, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864635b",
   "metadata": {},
   "source": [
    "Next, we have to convert the tensors into this dataset and dataloader objects in PyTorch, in order for the model to receive as inputs. Let us just run the code first, and we will take a look at the explanations later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b9e9886-962b-41b9-aeb5-acdbf451aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a70dc5",
   "metadata": {},
   "source": [
    "Let us again restrict our attention to just the ```train_dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cec16fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8591\n",
      "(tensor([  101,  9594,  3762,  1005,  1055,  2047,  1001,  9121,  1012,  1012,\n",
      "         1012,  8299,  1024,  1013,  1013,  1056,  1012,  2522,  1013, 22851,\n",
      "         8093,  2102,  2487,  2015,  2078,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor(0)) \n",
      "\n",
      "(tensor([  101,  1030,  4562, 12172,  3238, 13433,  2483,  1037, 29652, 20868,\n",
      "         2863,  1047,  1010,  2702,  6806,  1053,  8915,  9530,  7559,  8529,\n",
      "         3022,  2522, 14268,  2015,  1040,  2213,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), tensor(1)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "for i, batch in enumerate(train_dataset):\n",
    "    if i < 2:\n",
    "        print(batch, '\\n')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5688e1",
   "metadata": {},
   "source": [
    "The TensorDataset class is quite interesting... we can consider the following example from DataCamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ded5dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport torch\\nfrom torch.utils.data import TensorDataset\\n\\nnp_features = np.array(np.random.rand(12, 8))\\nnp_target = np.array(np.random.rand(12, 1))\\n\\n\\n# Convert arrays to PyTorch tensors\\ntorch_features = torch.tensor(np_features)\\ntorch_target = torch.tensor(np_target)\\n\\n# Create a TensorDataset from two tensors\\ndataset = TensorDataset(torch_features, torch_target)\\n\\n# Return the last element of this dataset\\ndataset[-1]\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "np_features = np.array(np.random.rand(12, 8))\n",
    "np_target = np.array(np.random.rand(12, 1))\n",
    "\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "torch_features = torch.tensor(np_features)\n",
    "torch_target = torch.tensor(np_target)\n",
    "\n",
    "# Create a TensorDataset from two tensors\n",
    "dataset = TensorDataset(torch_features, torch_target)\n",
    "\n",
    "# Return the last element of this dataset\n",
    "dataset[-1]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4b3d4",
   "metadata": {},
   "source": [
    "Next, we look at the ```train_loader```. The ```DataLoader``` is essentially an iterable over the dataset. You can use it split, transform and shuffle data on the fly. To know more about the DataLoader object, you can use the following link:\n",
    "\n",
    "https://stackoverflow.com/questions/65138643/examples-or-explanations-of-pytorch-dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c46f102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"Length of train_loader : {len(train_loader)}\")\\nfor i, batch in enumerate(train_loader):\\n    if i < 2:\\n        print(batch, \\'\\n\\')\\n    else:\\n        break\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(f\"Length of train_loader : {len(train_loader)}\")\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i < 2:\n",
    "        print(batch, '\\n')\n",
    "    else:\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6c916",
   "metadata": {},
   "source": [
    "We can also test this on the fake data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe8a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33577dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a00cb26",
   "metadata": {},
   "source": [
    "## Load BERT Model\n",
    "\n",
    "Here, we have to specify the BertModel that we want to use. we can use the ```BertForSequenceClassification``` attribute to load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d215bec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(bert_model_name).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a6f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d30a03e1-05a3-4687-a21e-87ff1dfe9ac5",
   "metadata": {},
   "source": [
    "## Train BERT Model\n",
    "\n",
    "Here is the where the real fun begins!! The following section is called the _training loop_\n",
    "\n",
    "For more information on different kinds of training, you can check out:\n",
    "\n",
    "https://huggingface.co/transformers/v4.4.2/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d312d2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Set the model \\'mode\\' to be in training\\nmodel.train()\\n\\nfor epoch in range(num_epoch):\\n    running_loss = 0.0\\n    correct = 0\\n    total = 0\\n    \\n    for step, batch in enumerate(train_loader):\\n        # Move batch tensors to the same device as the model\\n        input_ids, attention_mask, labels = [b.to(device) for b in batch]\\n        \\n        # Clears the gradients\\n        optimizer.zero_grad()\\n        \\n        # Forward pass\\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\\n        \\n        # Retrieves the loss\\n        loss = outputs.loss\\n        \\n        # Backward pass for gradient calculation\\n        loss.backward()\\n        \\n        # Updates the weights\\n        optimizer.step()\\n        \\n        # Accumulates the running loss\\n        running_loss += loss.item()\\n        \\n        # Predicts labels and calculates the number of correct predictions\\n        _, predicted = torch.max(outputs.logits, dim = 1)\\n        total += labels.size(0)\\n        correct += (predicted == labels).sum().item()\\n\\n    # Calculate accuracy and loss on the entire training set\\n    accuracy = correct / total\\n    average_loss = running_loss / len(train_loader)\\n\\n    # If the current epoch\\'s accuracy is best so far, save this model to disk\\n    if accuracy > best_accuracy:\\n        best_accuracy = accuracy\\n        torch.save(model, f\\'{model_dir}/best_model.pt\\')\\n\\n    print(f\"Epoch {epoch + 1}/{10} - Training Loss: {average_loss:.4f} - Training Accuracy: {accuracy:.4f}\")\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Set the model 'mode' to be in training\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Move batch tensors to the same device as the model\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        # Clears the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # Retrieves the loss\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass for gradient calculation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulates the running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Predicts labels and calculates the number of correct predictions\n",
    "        _, predicted = torch.max(outputs.logits, dim = 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate accuracy and loss on the entire training set\n",
    "    accuracy = correct / total\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # If the current epoch's accuracy is best so far, save this model to disk\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, f'{model_dir}/best_model.pt')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{10} - Training Loss: {average_loss:.4f} - Training Accuracy: {accuracy:.4f}\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e758418",
   "metadata": {},
   "source": [
    "We can only pick just 1 epoch, and at each step of the training process, get the loop to print out the outputs at the relevant portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e30ca394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids : \n",
      "tensor([[  101,  9594,  3762,  ...,     0,     0,     0],\n",
      "        [  101,  1030,  4562,  ...,     0,     0,     0],\n",
      "        [  101,  8822,  2739,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1021,  6739,  ...,     0,     0,     0],\n",
      "        [  101,  1030, 14426,  ...,     0,     0,     0],\n",
      "        [  101,  1030, 14255,  ...,     0,     0,     0]]), size : torch.Size([8, 128])\n",
      "\n",
      "Attention mask : \n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), size : torch.Size([8, 128])\n",
      "\n",
      "Labels : \n",
      "tensor([0, 1, 0, 1, 1, 0, 1, 1]), size : torch.Size([8])\n",
      "\n",
      "Outputs from the BERT Model : \n",
      "SequenceClassifierOutput(loss=tensor(0.5830, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3590,  0.3272],\n",
      "        [-0.1943,  0.1698],\n",
      "        [ 0.4817,  0.4054],\n",
      "        [ 0.0555,  0.2911],\n",
      "        [-0.0466,  0.4279],\n",
      "        [ 0.2699,  0.4091],\n",
      "        [-0.3140,  0.1387],\n",
      "        [-0.2212,  0.2627]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "loss from the outputs look like : \n",
      "0.5830422639846802\n",
      "\n",
      "Logits from the output : \n",
      "tensor([[ 0.3590,  0.3272],\n",
      "        [-0.1943,  0.1698],\n",
      "        [ 0.4817,  0.4054],\n",
      "        [ 0.0555,  0.2911],\n",
      "        [-0.0466,  0.4279],\n",
      "        [ 0.2699,  0.4091],\n",
      "        [-0.3140,  0.1387],\n",
      "        [-0.2212,  0.2627]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Outputs from the maximum of the logits : \n",
      "torch.return_types.max(\n",
      "values=tensor([0.3590, 0.1698, 0.4817, 0.2911, 0.4279, 0.4091, 0.1387, 0.2627],\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([0, 1, 0, 1, 1, 1, 1, 1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    # Calculate accuracy and loss on the entire training set\\n    accuracy = correct / total\\n    average_loss = running_loss / len(train_loader)\\n\\n    # If the current epoch\\'s accuracy is best so far, save this model to disk\\n    if accuracy > best_accuracy:\\n        best_accuracy = accuracy\\n        torch.save(model, f\\'{model_dir}/best_model.pt\\')\\n\\n    print(f\"Epoch {epoch + 1}/{10} - Training Loss: {average_loss:.4f} - Training Accuracy: {accuracy:.4f}\")\\n    '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model 'mode' to be in training\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        # Move batch tensors to the same device as the model\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "\n",
    "        # Clears the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # It is important the the input_ids and the attention_mask must have the same dimensions of (batch_size, max_length)\n",
    "        input_ids = input_ids.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "\n",
    "        print(f\"Input ids : \\n{input_ids}, size : {input_ids.size()}\\n\\nAttention mask : \\n{attention_mask}, size : {attention_mask.size()}\\n\\nLabels : \\n{labels}, size : {labels.size()}\\n\")\n",
    "\n",
    "        outputs = model(input_ids = input_ids, attention_mask = attention_mask, labels = labels)\n",
    "        print(f\"Outputs from the BERT Model : \\n{outputs}\\n\")\n",
    "\n",
    "        # Retrieves the loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        print(f\"loss from the outputs look like : \\n{loss}\\n\")\n",
    "        \n",
    "        # Backward pass for gradient calculation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updates the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulates the running loss\n",
    "        running_loss += loss\n",
    "        \n",
    "        # Retrieves the logits\n",
    "        logits = outputs.logits\n",
    "        print(f\"Logits from the output : \\n{logits}\\n\")\n",
    "\n",
    "        # Predicts labels and calculates the number of correct predictions\n",
    "        print(f\"Outputs from the maximum of the logits : \\n{torch.max(logits, dim = 1)}\")\n",
    "        _, predicted = torch.max(logits, dim = 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        break\n",
    "\n",
    "'''\n",
    "    # Calculate accuracy and loss on the entire training set\n",
    "    accuracy = correct / total\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # If the current epoch's accuracy is best so far, save this model to disk\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model, f'{model_dir}/best_model.pt')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{10} - Training Loss: {average_loss:.4f} - Training Accuracy: {accuracy:.4f}\")\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 1, 2, 3])\n",
    "x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ead0b728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 19, 14]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = [14, 17, 19]\n",
    "a2 = [11, 12, 14, 17, 18]\n",
    "a3 = [11, 12, 17, 18, 19]\n",
    "a4 = [11, 12, 14, 17, 18, 19]\n",
    "a5 = [11, 12, 17, 18, 19]\n",
    "\n",
    "a6 = [12, 14, 17, 18, 19, 21]\n",
    "a7 = [12, 13, 14, 17, 18, 19, 20, 21]\n",
    "a8 = [11, 12, 14, 17, 18, 19]\n",
    "a9 = [13, 14, 17, 19, 20]\n",
    "\n",
    "# Only for ODI Team members\n",
    "common_dates = list(set.intersection(*map(set, [a6, a7, a8, a9])))\n",
    "common_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ead0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3cb5cac",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f36a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Variables to gather full output\n",
    "total_eval_accuracy = 0\n",
    "total_eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_loader:\n",
    "    # Unpack this training batch from our dataloader and move tensors to GPU if available\n",
    "    input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "    \n",
    "    # Tells PyTorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training)\n",
    "    with torch.no_grad():        \n",
    "        # Forward pass, calculate logit predictions.\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    # Get the loss and logits\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Accumulate the validation loss\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches\n",
    "    _, predictions = torch.max(logits, dim=1)\n",
    "    total_eval_accuracy += (predictions == labels).sum().item()\n",
    "\n",
    "# Report the final accuracy for this validation run\n",
    "avg_val_accuracy = total_eval_accuracy / len(test_loader.dataset)\n",
    "print(\"Accuracy on the test set: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "# Calculate the average loss over all of the batches\n",
    "avg_val_loss = total_eval_loss / len(test_loader)\n",
    "print(\"Test Loss: {0:.2f}\".format(avg_val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51924f2a-ce13-4200-8cce-b58e72174725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948abb5-9fb7-45a1-b8ff-7bbc7173e560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55895f85-be0d-4b55-ac4c-b99fd6101f79",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684c763-37b9-4c45-bcda-21f3e19911fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bf29b-870d-499b-a565-6f77c5202f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafb898-f469-4045-b1b9-12639144015e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
