{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce280679-aee9-4c41-97c0-f6e8a354f401",
   "metadata": {},
   "source": [
    "# PyTorch for Natural Language Processing\n",
    "\n",
    "## Bot Detection using BERTModel\n",
    "\n",
    "---\n",
    "\n",
    "**<u>_Objective:_</u>** In this short project, we fine-tune a BERT pretrained model to classify tweets made by a bot, or by a human.\n",
    "\n",
    "This tutorial is inspired by the following walkthrough:\n",
    "\n",
    "https://saturncloud.io/blog/pytorch-for-natural-language-processing-building-a-fake-news-classification-model/\n",
    "\n",
    "\n",
    "### Introduction \n",
    "\n",
    "Bot detection lols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ab01a-65ee-42dd-a9e6-9dc3fd4fd3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb68be-df1f-454c-b327-0690038843a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c873fb4d-c6d1-4b41-8897-8bc16a9fb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies and libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_theme(style = 'whitegrid', \n",
    "              rc    = {'figure.dpi'    : 400, \n",
    "                       'figure.figsize': (20, 12)}, \n",
    "              font_scale = 0.60)\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "warnings.filterwarnings('ignore', category = UserWarning, module = 'openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a7bca-33ed-44ec-829a-f7d18e808f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b491c0bf-d7e6-4345-9632-e9a7e83258f9",
   "metadata": {},
   "source": [
    "## Set up Environment for Google Colab\n",
    "\n",
    "You would need to have GPU to run the fine-tuning. Google Colab is a great platform for using this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0f14b-019f-48ef-b154-a22e78af6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384eae10-c2fb-4b02-b893-564e6e9bee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: i:\\My Drive\\Data Science and Analytics Portfolio\\3 Tutorials\\2_Bots_Detection\\2_Notebooks\n",
      "Root directory : i:\\My Drive\\Data Science and Analytics Portfolio\\3 Tutorials\\2_Bots_Detection\\\n",
      "Data directory : i:\\My Drive\\Data Science and Analytics Portfolio\\3 Tutorials\\2_Bots_Detection\\1_Data\\\n",
      "Model directory : i:\\My Drive\\Data Science and Analytics Portfolio\\3 Tutorials\\2_Bots_Detection\\3_Model\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get current and root directory\n",
    "cur_dir = os.getcwd()\n",
    "root_dir = cur_dir[:-11]\n",
    "data_dir = root_dir + \"1_Data\\\\\"\n",
    "model_dir = root_dir + \"3_Model\\\\\"\n",
    "\n",
    "print(f\"Current directory: {cur_dir}\\nRoot directory : {root_dir}\\nData directory : {data_dir}\\nModel directory : {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db240a6b-d127-4470-adce-43afec715e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "619a21bc-d1f2-4841-a5d4-aedfec4b93b6",
   "metadata": {},
   "source": [
    "## Read Datasets\n",
    "\n",
    "For the purpose of this project, we wil only take a sample of about 5000 tweets from both dataframes, so that the training does not take too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11db2ba2-e587-4bbd-aa62-be93987a5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_human_temp = pd.read_csv(f\"{data_dir}/cresci-2015/TFP_tweets.csv\", encoding = 'latin-1')\n",
    "df_bot_temp = pd.read_csv(f\"{data_dir}/cresci-2015/TWT_tweets.csv\", encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6797d47-94ab-445c-bd52-f086c215ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of human dataframe : 563693\n",
      "Legnth of bots dataframe : 114192\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of human dataframe : {len(df_human_temp)}\\nLegnth of bots dataframe : {len(df_bot_temp)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7657d4-e081-4c5a-a93c-5244fb035e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9975cc20-5480-4718-84b8-cda446590c86",
   "metadata": {},
   "source": [
    "The size of the two datasets are very huge. What we can do, is to split them up into two datasets - one that will go into the train-test-split function, and another that will be used for model prediction. For the former, we only select about 5000 rows of them, as so that the model does not take too long to train. Generally speaking, the more data, the better the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a10a3b87-f05b-419a-b39c-cb9c243b64e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of human sample : 5000\n",
      "Legnth of bots sample : 6000\n",
      "Length of train dataframe : 11000\n",
      "Length of evaluation dataframe : 1900\n"
     ]
    }
   ],
   "source": [
    "## Split into training data\n",
    "df_human_train_sample = df_human_temp.sample(n = 5000, random_state = 37)\n",
    "df_bot_train_sample = df_bot_temp.sample(n = 6000, random_state = 37)\n",
    "\n",
    "# Those tweets that are not inside the training dataset, are for predictions. Again, we only pick about 1000\n",
    "df_human_eval =  df_human_temp[~df_human_temp['id'].isin(df_human_train_sample['id'].values)]\n",
    "df_bot_eval = df_bot_temp[~df_bot_temp['id'].isin(df_bot_train_sample['id'].values)]\n",
    "\n",
    "# Select only 1000 tweets for the evaluation dataset\n",
    "df_human_eval_sample = df_human_eval.sample(n = 1000, random_state = 30)\n",
    "df_bot_eval_sample = df_bot_eval.sample(n = 900, random_state = 30)\n",
    "\n",
    "# Then we only select the relevant columns and encode the humans as 0 and bots as 1\n",
    "df_human_train_sample['target'] = 0\n",
    "df_bot_train_sample['target'] = 1\n",
    "\n",
    "df_human_eval_sample['target'] = 0\n",
    "df_bot_eval_sample['target'] = 1\n",
    "\n",
    "# Vstack the dataframes together, and randomly shuffle them dataframe\n",
    "df_train_sample = pd.concat([df_human_train_sample, df_bot_train_sample], axis = 0, ignore_index = True) \n",
    "df_eval_sample = pd.concat([df_human_eval_sample, df_bot_eval_sample], axis = 0, ignore_index = True) \n",
    "\n",
    "df_train_sample = df_train_sample.sample(frac = 1.0, random_state = 90)\n",
    "df_eval_sample = df_eval_sample.sample(frac = 1.0, random_state = 90)\n",
    "\n",
    "\n",
    "print(f\"Length of human sample : {len(df_human_train_sample)}\\nLegnth of bots sample : {len(df_bot_train_sample)}\\nLength of train dataframe : {len(df_train_sample)}\")\n",
    "print(f\"Length of evaluation dataframe : {len(df_eval_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b319882-7ee5-470a-b507-12ee01f449c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b4034ea-af94-4328-b429-b8aff358fd78",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Usually, we want to perform some rudimentary data cleaning steps on the dataset before we use it for training. Typically, this involves:\n",
    "- Removing special characters\n",
    "- Lower case all letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a46898c-b1db-4200-9c48-c15401eb7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    \n",
    "    tweet1 = re.sub('[^A-Za-z0-9]+', ' ', tweet)\n",
    "    tweet2 = tweet1.lower()\n",
    "    tweet3 = tweet2.strip()\n",
    "    \n",
    "    return tweet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f66d0ae-3e6c-4de2-b11a-b3081b26a855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2217</th>\n",
       "      <td>179273485179297793</td>\n",
       "      <td>#FiatRom Caro Marchionne, il capitale e' sempr...</td>\n",
       "      <td>0</td>\n",
       "      <td>fiatrom caro marchionne il capitale e sempre s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>290532928440639488</td>\n",
       "      <td>Roma/ A #Ostia si continua a sparare e la poli...</td>\n",
       "      <td>0</td>\n",
       "      <td>roma a ostia si continua a sparare e la polizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7656</th>\n",
       "      <td>327948536136204288</td>\n",
       "      <td>http://t.co/ivA8bzYixB ÑÐ°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿...</td>\n",
       "      <td>1</td>\n",
       "      <td>http t co iva8bzyixb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>292313083421011970</td>\n",
       "      <td>@Diabolikart Quoto su tutta la linea ;-))) bac...</td>\n",
       "      <td>0</td>\n",
       "      <td>diabolikart quoto su tutta la linea bacio e ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>110463456469188608</td>\n",
       "      <td>@midbrito foi na pria ontem&amp;gt;?</td>\n",
       "      <td>1</td>\n",
       "      <td>midbrito foi na pria ontem gt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10919</th>\n",
       "      <td>301939393281798144</td>\n",
       "      <td>TODO MEXICO APOYA Y PIDE  SE RESPETE LA DECISI...</td>\n",
       "      <td>1</td>\n",
       "      <td>todo mexico apoya y pide se respete la decisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9539</th>\n",
       "      <td>13304808760938497</td>\n",
       "      <td>@sigatchegarotos queria dar os parabÃ©ns a vcs...</td>\n",
       "      <td>1</td>\n",
       "      <td>sigatchegarotos queria dar os parab ns a vcs p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>135782553570385920</td>\n",
       "      <td>@euphonik thank u for the brilliant tracks !!!...</td>\n",
       "      <td>1</td>\n",
       "      <td>euphonik thank u for the brilliant tracks help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>311160499553333248</td>\n",
       "      <td>RT @WIPO: Free ePCT webinars: Learn how to man...</td>\n",
       "      <td>0</td>\n",
       "      <td>rt wipo free epct webinars learn how to manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3931</th>\n",
       "      <td>125867823514910720</td>\n",
       "      <td>Il novanta per cento della fantascienza ÃÂ¨ s...</td>\n",
       "      <td>0</td>\n",
       "      <td>il novanta per cento della fantascienza spazza...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10739 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "2217   179273485179297793  #FiatRom Caro Marchionne, il capitale e' sempr...   \n",
       "3387   290532928440639488  Roma/ A #Ostia si continua a sparare e la poli...   \n",
       "7656   327948536136204288  http://t.co/ivA8bzYixB ÑÐ°ÑÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿...   \n",
       "1833   292313083421011970  @Diabolikart Quoto su tutta la linea ;-))) bac...   \n",
       "7497   110463456469188608                   @midbrito foi na pria ontem&gt;?   \n",
       "...                   ...                                                ...   \n",
       "10919  301939393281798144  TODO MEXICO APOYA Y PIDE  SE RESPETE LA DECISI...   \n",
       "9539    13304808760938497  @sigatchegarotos queria dar os parabÃ©ns a vcs...   \n",
       "6815   135782553570385920  @euphonik thank u for the brilliant tracks !!!...   \n",
       "2717   311160499553333248  RT @WIPO: Free ePCT webinars: Learn how to man...   \n",
       "3931   125867823514910720  Il novanta per cento della fantascienza ÃÂ¨ s...   \n",
       "\n",
       "       target                                       text_cleaned  \n",
       "2217        0  fiatrom caro marchionne il capitale e sempre s...  \n",
       "3387        0  roma a ostia si continua a sparare e la polizi...  \n",
       "7656        1                               http t co iva8bzyixb  \n",
       "1833        0  diabolikart quoto su tutta la linea bacio e ne...  \n",
       "7497        1                      midbrito foi na pria ontem gt  \n",
       "...       ...                                                ...  \n",
       "10919       1  todo mexico apoya y pide se respete la decisio...  \n",
       "9539        1  sigatchegarotos queria dar os parab ns a vcs p...  \n",
       "6815        1  euphonik thank u for the brilliant tracks help...  \n",
       "2717        0  rt wipo free epct webinars learn how to manage...  \n",
       "3931        0  il novanta per cento della fantascienza spazza...  \n",
       "\n",
       "[10739 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train_sample[['id', 'text', 'target']].copy()\n",
    "df_eval = df_eval_sample[['id', 'text', 'target']].copy()\n",
    "\n",
    "df_train['text_cleaned'] = df_train['text'].apply(lambda x: clean_text(x)) \n",
    "df_eval['text_cleaned'] = df_eval['text'].apply(lambda x: clean_text(x)) \n",
    "\n",
    "# Remove any rows that are blanks\n",
    "df_train = df_train[df_train['text_cleaned'] != '']\n",
    "df_eval = df_eval[df_eval['text_cleaned'] != '']\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca94ec60-d5ed-4d18-b25b-0b56e26f06f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5739\n",
       "0    5000\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0d24e1c-fbfb-4b42-ba2d-459e9c4b0fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    999\n",
       "1    858\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b76010-ca8f-4e77-a79c-3785ad447580",
   "metadata": {},
   "source": [
    "Now, we are ready to send the dataframe into the train-test-split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41741a59-b080-4d79-8d5a-7e2094c200b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train : 8591, Length of X_test: 2148\n",
      "Length of y_train : 8591, Length of y_test: 2148\n"
     ]
    }
   ],
   "source": [
    "texts = df_train['text'].values\n",
    "targets = df_train['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, targets, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(f\"Length of X_train : {len(X_train)}, Length of X_test: {len(X_test)}\\nLength of y_train : {len(y_train)}, Length of y_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d808c-1c8e-4352-8db5-825089902219",
   "metadata": {},
   "source": [
    "As we can see, the lengths of the features (X) and their corresponding labels (y) are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bff494-745e-47ca-9728-8c4329bc7dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcbd11-8793-4d2f-8b28-0efa21e876a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97eb1f57-1391-437f-9730-ce24761bdeb0",
   "metadata": {},
   "source": [
    "## Parameters Declarations\n",
    "\n",
    "In this section, we want to declare some variables that we will need in our model training latter\n",
    "\n",
    "There are many bert models you can try. See the list of models hosted by Hugging Face on the link below:\n",
    "\n",
    "https://huggingface.co/google-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de642731-e7b7-4bba-bb2b-1925e8490067",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-uncased'  # Name of the bert model\n",
    "batch_size      = 8                    # Size of each batch that the dataloader will send into BERT for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62fbeb-9807-4720-83a1-ea5d40670252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430f71f-aac2-4546-844f-90f2c22251e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef599042-26e1-460f-be8f-36d7cdda2e94",
   "metadata": {},
   "source": [
    "## Load BERT Model\n",
    "\n",
    "We need to have a GPU to push the model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465d636e-9afa-4cd3-ae85-c449eceb53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu \n",
      "Name of device: CPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "try:\n",
    "    device_name = torch.cuda.get_device_name()\n",
    "except:\n",
    "    device_name = 'CPU'\n",
    "print(device, '\\nName of device:', device_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891503e6-3ead-48cb-9be9-7a33fdbde5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6d34cd3-f32c-4933-b370-78367ccd1469",
   "metadata": {},
   "source": [
    "## Tokenize Texts\n",
    "\n",
    "In order for BERT to perform its embeddings and classification, we have to split the sentences into individual words - or _tokens_. In BERT, there is a tokenizer we can use to do just this. Stil step of tokenization will take some time, depending on the compute RAM as well as how long the sentences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f4c338-2980-45a1-88eb-ba4b01ac928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case = True)\n",
    "\n",
    "train_encodings = tokenizer(list(X_train), truncation = True, padding = True, max_length = 128) \n",
    "test_encodings = tokenizer(list(X_test), truncation = True, padding = True, max_length = 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a396749-d32a-41c6-8c09-566df645abb2",
   "metadata": {},
   "source": [
    "We can also check what does the tokenizer returns us. Let's pick the ```train_encodings```. The encodings actually return us a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f310325-ce34-46c7-8dbb-9b6d635fbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Length of input_ids : 8591\n",
      "Length of token_type_ids : 8591\n",
      "Length of attention_mask : 8591\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings.keys())\n",
    "for key in train_encodings.keys():\n",
    "    print(f\"Length of {key} : {len(train_encodings[key])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501acd61-9073-4715-850a-4d15819fc181",
   "metadata": {},
   "source": [
    "Note that this is exactly the same length of X_train! We can examine each of the keys and see what do they represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6004953-20ee-470c-9d57-bb10c9e09066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each input_ids : 128\n",
      "[101, 9594, 3762, 1005, 1055, 2047, 1001, 9121, 1012, 1012, 1012, 8299, 1024, 1013, 1013, 1056, 1012, 2522, 1013, 22851, 8093, 2102, 2487, 2015, 2078, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of each input_ids : {len(train_encodings['input_ids'][0])}\\n{train_encodings['input_ids'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4aee01",
   "metadata": {},
   "source": [
    "Each of the ```input_ids``` are exactly 128 long - this is because we have set such as value a the ```max_length``` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a01dd5e7-6015-4b35-8f53-e5b9f28c401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each token_type_ids : 128\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of each token_type_ids : {len(train_encodings['token_type_ids'][0])}\\n{train_encodings['token_type_ids'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e276e",
   "metadata": {},
   "source": [
    "lastly, we have the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b636cd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each token_type_ids : 128\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of each token_type_ids : {len(train_encodings['attention_mask'][0])}\\n{train_encodings['attention_mask'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ba602",
   "metadata": {},
   "source": [
    "Actaully, out of the three keys, only ```input_ids``` and ```attention_mask``` are required, as well as the target values, for fine-tuning the model. Next, we create another function to convert the lists to PyTorch Tensors, and sending them to the GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b01c6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tensor_device(encodings, target, device):\n",
    "\n",
    "    inputs = torch.tensor(encodings['input_ids']).to(device)\n",
    "    masks = torch.tensor(encodings['attention_mask']).to(device)\n",
    "    labels = inputs = torch.tensor(target).to(device)\n",
    "    return inputs, masks, labels\n",
    "\n",
    "\n",
    "# Function call\n",
    "train_inputs, train_masks, train_labels = convert_tensor_device(train_encodings, y_train, device)\n",
    "test_inputs, test_masks, test_labels = convert_tensor_device(test_encodings, y_test, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4466b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d864635b",
   "metadata": {},
   "source": [
    "Next, we have to convert the tensors into this dataset and dataloader objects in PyTorch, in order for the model to receive as inputs. Let us just run the code first, and we will take a look at the explanations later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b9e9886-962b-41b9-aeb5-acdbf451aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a70dc5",
   "metadata": {},
   "source": [
    "Let us again restrict our attention to just the ```train_dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec16fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46f102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d30a03e1-05a3-4687-a21e-87ff1dfe9ac5",
   "metadata": {},
   "source": [
    "## Train BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51924f2a-ce13-4200-8cce-b58e72174725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948abb5-9fb7-45a1-b8ff-7bbc7173e560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55895f85-be0d-4b55-ac4c-b99fd6101f79",
   "metadata": {},
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684c763-37b9-4c45-bcda-21f3e19911fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bf29b-870d-499b-a565-6f77c5202f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbafb898-f469-4045-b1b9-12639144015e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
