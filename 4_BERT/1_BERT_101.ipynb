{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e69d55-792c-491d-8430-9a98541aa566",
   "metadata": {},
   "source": [
    "# BERT Tutorial: BERT 101 - State Of The Art NLP Model Explained\n",
    "\n",
    "#### _<ins>**Objective</ins>:**_ \n",
    "\n",
    "Get a high level understanding of BERTModel\n",
    "\n",
    "\n",
    "This notebook follows the following link: https://huggingface.co/blog/bert-101\n",
    "\n",
    "#### **<ins>Introduction:</ins>**\n",
    "\n",
    "We will get a very high level understanding of BERT Model.\n",
    "\n",
    "#### **<ins>What is BERT?</ins>**\n",
    "\n",
    "BERT stands for _Bidirectional Encoder Representations from Transformers._ Developed in 2018 by Google AI Language, the model is considered state-of-the-art and a real game changer in the world of natural language procesing (NLP). Typically, in solving problems in NLP, one requires a specific model for a specific problem. \n",
    "\n",
    "A key challenge in NLP is the ability for computers to understand the language context. For example, consider the following two sentences:\n",
    "\n",
    "<center>\n",
    "    I went to the bank <br>\n",
    "    I went to the river bank\n",
    "</center>\n",
    "\n",
    "to humans, the word *bank* here have rather different meangings - as one refers to be the bank where you deposit your money, the other refers to the edge of the river. For computers, much work has been put so that they can distinguish the meaning based on the context words.\n",
    "\n",
    "In fact, BERT has been benchmarked with other models using the GLUE and has always shown to outperform the rest.\n",
    "\n",
    "\n",
    "#### **<ins>How Does BERT Work?</ins>**\n",
    "\n",
    "##### <ins>1. Large Amounts of Training Data</ins>\n",
    "\n",
    "BERT was trained on Wikipedia and Google's BooksCorpus data - both amounting to about 3.3 billion words. 64 Tensor Processing Units (TPUs) trained BERT over the course of 4 days. \n",
    "\n",
    "Of course, BERT Model is very large. There are smaller BERT Models of varying sizes that can be used for smaller embedding tasks. You may check out these smaller models at https://github.com/google-research/bert and https://huggingface.co/docs/transformers/model_doc/distilbert. \n",
    "\n",
    "\n",
    "##### <ins>2. Using Masked Language Modeling</ins>\n",
    "\n",
    "Masked Lanuage Model enforces birectional learning from text by masking (hiding) a word in a sentence, and forcing BERT to use the words on both sides of the masked word to guess what it means. \n",
    "\n",
    "\n",
    "##### <ins>3. Next Sentence Prediction</ins>\n",
    "\n",
    "This method is used to help BERT learning about relationships between sentences by predicint if a given sentence follows the previous sentence or not. \n",
    "\n",
    "\n",
    "##### <ins>4. Transformers</ins>\n",
    "\n",
    "The unique aspect of Transformers is the usage of an attention mechanism, to observe relationships between words. This concept was first introduced in 2017, and it revolutionalized the domain of NLP. You may check out the paper in https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. \n",
    "\n",
    "We can roughly outline how this attention layer works. This is not too different from how we humans process information - we tend to forget details about insignificant or trivial events, and focus on the important ones. Similarly, the attention mechanism allows machine learning models to pay attention to significant details, and ignore irrelevant information. Transformers create differential weights on the words, hinting which words are more critical to further process. \n",
    "\n",
    "A transformer accomplish this by successively processing an input through a stack of transformer layers, called the encoder. If required, an additional stack of layers called a decoder can be used to predict a target output. Howevever, BERT does not employ one. \n",
    "\n",
    "\n",
    "#### **<ins>BERT Model Size and Architecture</ins>**\n",
    "\n",
    "The two original BERT models are called BERTbase and BERTlarge:\n",
    "\n",
    "\n",
    "|           | Transformer Layers | Hidden Size | Attention Heads | Parameters  | Processing | Length of Training  |\n",
    "| :-------: | :----------------: | :---------: |:--------------: |:----------: |:---------: |:------------------: |\n",
    "| BERTbase  |          12        |   768       |      12         | 110 Million |   4 TPUs   |      4 days         |\n",
    "| BERTlarge |          24        |   1024      |      16         | 340 Million |  16 TPUs   |      4 days         |\n",
    "\n",
    "\n",
    "#### **<ins>What Makes BERT Special?</ins>**\n",
    "- BERT was trained on massive amounts of unlabeled data (no human annotation) in an unsupervised fashion.\n",
    "- BERT was then trained on small amounts of human-annotated data starting from the previous pre-trained model resulting in state-of-the-art performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404fc578-5e37-45e9-8eb0-d8ffe2770a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a979f2f-0b9a-40e0-84ad-df570321c794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc15c7-5e39-4fdc-8eae-951cba8c2042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e65d1b-4efa-4526-a698-56aa98553892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a7fa3-aff4-448c-a010-bb6064f834f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718382b5-8cc1-430d-8e2e-40f1bc1fb45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b1939-c324-4ef2-864a-1131415046b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
